,f1_macro,recall_macro,precision_macro,accuracy,auc_score,vectorizer
0,0.8677028571428571,0.868347619047619,0.8697514285714285,0.8678438095238094,0.9350504761904763,CountVectorizer(tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fc70a2873a0>>)
1,0.8716619047619046,0.8723266666666667,0.874092380952381,0.8718304761904762,0.9381590476190476,"CountVectorizer(ngram_range=(1, 3),
                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fdd4bb6f5e0>>)"
2,0.8582742857142858,0.8586895238095239,0.8592333333333332,0.8583600000000001,0.9252038095238095,"CountVectorizer(analyzer='char', ngram_range=(1, 5))"
3,0.8128628571428573,0.8137295238095239,0.8153742857142857,0.8131219047619047,0.8932333333333333,"CountVectorizer(analyzer='char', ngram_range=(4, 5))"
4,0.817944761904762,0.8186085714285715,0.8196057142857143,0.8181066666666665,0.8975761904761905,"CountVectorizer(analyzer='char', ngram_range=(3, 8))"
5,0.8823304761904762,0.8825152380952381,0.883357142857143,0.8824609523809523,0.9479114285714284,TfidfVectorizer(tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f03948dc3a0>>)
6,0.86978,0.8701171428571429,0.8754257142857143,0.8705561904761906,0.9469285714285713,"TfidfVectorizer(ngram_range=(1, 3),
                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f7d6df5f3a0>>)"
7,0.8754095238095239,0.8755752380952381,0.8760485714285715,0.8755247619047619,0.9433609523809523,"TfidfVectorizer(analyzer='char', ngram_range=(1, 5))"
8,0.8459304761904763,0.846172380952381,0.8468285714285715,0.8460714285714288,0.923292380952381,"TfidfVectorizer(analyzer='char', ngram_range=(4, 5))"
9,0.8475961904761904,0.8477390476190475,0.8495133333333335,0.847915238095238,0.9261019047619048,"TfidfVectorizer(analyzer='char', ngram_range=(3, 8))"
