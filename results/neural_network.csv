,f1_macro,recall_macro,precision_macro,accuracy,auc_score,vectorizer
0,0.8758,0.8756,0.8776,0.876,0.9286,"['CountVectorizer(max_features=10000,\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)', 'CountVectorizer(max_features=10000,\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)', 'CountVectorizer(max_features=10000,\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)', 'CountVectorizer(max_features=10000,\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)', 'CountVectorizer(max_features=10000,\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)']"
1,0.8513999999999999,0.85145,0.85375,0.85165,0.9160999999999999,"['CountVectorizer(max_features=10000, ngram_range=(1, 3),\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)', 'CountVectorizer(max_features=10000, ngram_range=(1, 3),\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)', 'CountVectorizer(max_features=10000, ngram_range=(1, 3),\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)', 'CountVectorizer(max_features=10000, ngram_range=(1, 3),\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)', 'CountVectorizer(max_features=10000, ngram_range=(1, 3),\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)']"
2,0.8416,0.8424666666666667,0.844,0.8417666666666667,0.9115666666666665,"[""CountVectorizer(analyzer='char', max_features=10000, ngram_range=(1, 5))"", ""CountVectorizer(analyzer='char', max_features=10000, ngram_range=(1, 5))"", ""CountVectorizer(analyzer='char', max_features=10000, ngram_range=(1, 5))"", ""CountVectorizer(analyzer='char', max_features=10000, ngram_range=(1, 5))"", ""CountVectorizer(analyzer='char', max_features=10000, ngram_range=(1, 5))""]"
3,0.830075,0.830825,0.832025,0.8302,0.9044249999999999,"[""CountVectorizer(analyzer='char', max_features=10000, ngram_range=(4, 5))"", ""CountVectorizer(analyzer='char', max_features=10000, ngram_range=(4, 5))"", ""CountVectorizer(analyzer='char', max_features=10000, ngram_range=(4, 5))"", ""CountVectorizer(analyzer='char', max_features=10000, ngram_range=(4, 5))"", ""CountVectorizer(analyzer='char', max_features=10000, ngram_range=(4, 5))""]"
4,0.82534,0.82642,0.82796,0.8255000000000001,0.9009199999999999,"[""CountVectorizer(analyzer='char', max_features=10000, ngram_range=(3, 8))"", ""CountVectorizer(analyzer='char', max_features=10000, ngram_range=(3, 8))"", ""CountVectorizer(analyzer='char', max_features=10000, ngram_range=(3, 8))"", ""CountVectorizer(analyzer='char', max_features=10000, ngram_range=(3, 8))"", ""CountVectorizer(analyzer='char', max_features=10000, ngram_range=(3, 8))""]"
5,0.8286666666666666,0.8295833333333333,0.8309833333333333,0.8288166666666666,0.9068333333333333,"['TfidfVectorizer(max_features=10000,\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)', 'TfidfVectorizer(max_features=10000,\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)', 'TfidfVectorizer(max_features=10000,\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)', 'TfidfVectorizer(max_features=10000,\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)', 'TfidfVectorizer(max_features=10000,\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)']"
6,0.8315571428571429,0.8322714285714286,0.8339142857142857,0.8317714285714286,0.9114285714285714,"['TfidfVectorizer(max_features=10000, ngram_range=(1, 3),\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)', 'TfidfVectorizer(max_features=10000, ngram_range=(1, 3),\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)', 'TfidfVectorizer(max_features=10000, ngram_range=(1, 3),\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)', 'TfidfVectorizer(max_features=10000, ngram_range=(1, 3),\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)', 'TfidfVectorizer(max_features=10000, ngram_range=(1, 3),\n                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f688e5ba850>>)']"
7,0.8305374999999999,0.8311625,0.8326,0.830725,0.9112374999999999,"[""TfidfVectorizer(analyzer='char', max_features=10000, ngram_range=(1, 5))"", ""TfidfVectorizer(analyzer='char', max_features=10000, ngram_range=(1, 5))"", ""TfidfVectorizer(analyzer='char', max_features=10000, ngram_range=(1, 5))"", ""TfidfVectorizer(analyzer='char', max_features=10000, ngram_range=(1, 5))"", ""TfidfVectorizer(analyzer='char', max_features=10000, ngram_range=(1, 5))""]"
8,0.8314222222222222,0.8319666666666667,0.8333666666666667,0.8316222222222223,0.9122777777777777,"[""TfidfVectorizer(analyzer='char', max_features=10000, ngram_range=(4, 5))"", ""TfidfVectorizer(analyzer='char', max_features=10000, ngram_range=(4, 5))"", ""TfidfVectorizer(analyzer='char', max_features=10000, ngram_range=(4, 5))"", ""TfidfVectorizer(analyzer='char', max_features=10000, ngram_range=(4, 5))"", ""TfidfVectorizer(analyzer='char', max_features=10000, ngram_range=(4, 5))""]"
9,0.8385199999999999,0.8390299999999999,0.8402900000000001,0.8387,0.91661,"[""TfidfVectorizer(analyzer='char', max_features=10000, ngram_range=(3, 8))"", ""TfidfVectorizer(analyzer='char', max_features=10000, ngram_range=(3, 8))"", ""TfidfVectorizer(analyzer='char', max_features=10000, ngram_range=(3, 8))"", ""TfidfVectorizer(analyzer='char', max_features=10000, ngram_range=(3, 8))"", ""TfidfVectorizer(analyzer='char', max_features=10000, ngram_range=(3, 8))""]"
