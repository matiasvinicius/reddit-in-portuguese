{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "To use many and diverse models from scikit-learn, we build the AuthorClassifier class, containing the base structure to use the machine learning models. \n",
    "\n",
    "This class has a constructor method that receives a machine learning model and a vectorizer from sklearn, and can receive to a scaler object (like MinMaxScaler or StandardScaler) and a PCA object. This 4 elements will define the steps of the pipeline, that is created using the fit method.\n",
    "\n",
    "predict method gives the predicted classes and store the predictions probabilities, because this is usefull to calculate the AUC score. evaluate method calculates (from a binary problem so far) the metrics of accuracy, precision, recall, F1-score and AUC ROC. For precision, recall and F1-score is calculated the metrics based on macro, micro, weighted and for each class.\n",
    "\n",
    "Obs: Given that TfidfVectorizer and CountVectorizer output is a sparse matrix, when we want to calculate PCA or change the scale we need to tranform this features from sparse to dense, and the class SparseToArray on utils.py file do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../../libs')\n",
    "from utils import get_data\n",
    "from autorship import AuthorClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(\"../../data/authors.csv\")\n",
    "data = data[data.username.isin(data.username.unique()[:2])] #select two authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author1': 'BluePirate89', 'author2': 'Manada_2', 'precision_author1': 0.8471, 'recall_author1': 0.9028, 'f1_score_author1': 0.8741, 'precision_author2': 0.899, 'recall_author2': 0.8415, 'f1_score_author2': 0.8693, 'precision_weighted': 0.8734, 'precision_micro': 0.8717, 'precision_macro': 0.873, 'recall_weighted': 0.8717, 'recall_micro': 0.8717, 'recall_macro': 0.8721, 'f1_weighted': 0.8716, 'f1_micro': 0.8717, 'f1_macro': 0.8717, 'auc_score': 0.9505, 'accuracy': 0.8717}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.comment, data.username, test_size=0.33, random_state=42)\n",
    "    \n",
    "clf = AuthorClassifier(clf=LogisticRegression())\n",
    "pipe = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(clf.evaluate(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(\"../../data/authors.csv\")\n",
    "data = data[data.username.isin(data.username.unique()[10:12])] #select two authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('TfidfVectorizer()', TfidfVectorizer()),\n",
      "                ('SparseToArray()', SparseToArray()),\n",
      "                ('MinMaxScaler()', MinMaxScaler()),\n",
      "                ('PCA(n_components=0.95)', PCA(n_components=0.95)),\n",
      "                ('RandomForestClassifier()', RandomForestClassifier())]) \n",
      "\n",
      "{'author1': 'CariocaSatanico', 'author2': 'xanax101010', 'precision_author1': 0.7567, 'recall_author1': 0.9688, 'f1_score_author1': 0.8497, 'precision_author2': 0.958, 'recall_author2': 0.6951, 'f1_score_author2': 0.8057, 'precision_weighted': 0.8584, 'precision_micro': 0.8305, 'precision_macro': 0.8573, 'recall_weighted': 0.8305, 'recall_micro': 0.8305, 'recall_macro': 0.832, 'f1_weighted': 0.8275, 'f1_micro': 0.8305, 'f1_macro': 0.8277, 'auc_score': 0.9522, 'accuracy': 0.8305}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.comment, data.username, test_size=0.33, random_state=42)\n",
    "    \n",
    "clf = AuthorClassifier(vectorizer=TfidfVectorizer(),\n",
    "                        clf=RandomForestClassifier(), \n",
    "                        scaler=MinMaxScaler(),\n",
    "                        pca=PCA(n_components=0.95))\n",
    "pipe = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(pipe,\"\\n\")\n",
    "print(clf.evaluate(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c47aa80a392bf6b860d7beb7b265756c1d97ffcd6d5effdf04ac88dd49d67208"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('reddit-in-portuguese-CNZVnMpw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
